:py:mod:`gpflux.layers.basis_functions.random_fourier_features`
===============================================================

.. py:module:: gpflux.layers.basis_functions.random_fourier_features

.. autoapi-nested-parse::

   A kernel's features and coefficients using Random Fourier Features (RFF). 



Module Contents
---------------

.. py:class:: RandomFourierFeaturesBase(kernel: gpflow.kernels.Kernel, output_dim: int, **kwargs: Mapping)

   Bases: :py:obj:`tensorflow.keras.layers.Layer`

   This is the class from which all layers inherit.

   A layer is a callable object that takes as input one or more tensors and
   that outputs one or more tensors. It involves *computation*, defined
   in the `call()` method, and a *state* (weight variables), defined
   either in the constructor `__init__()` or in the `build()` method.

   Users will just instantiate a layer and then treat it as a callable.

   Args:
     trainable: Boolean, whether the layer's variables should be trainable.
     name: String name of the layer.
     dtype: The dtype of the layer's computations and weights. Can also be a
       `tf.keras.mixed_precision.Policy`, which allows the computation and weight
       dtype to differ. Default of `None` means to use
       `tf.keras.mixed_precision.global_policy()`, which is a float32 policy
       unless set to different value.
     dynamic: Set this to `True` if your layer should only be run eagerly, and
       should not be used to generate a static computation graph.
       This would be the case for a Tree-RNN or a recursive network,
       for example, or generally for any layer that manipulates tensors
       using Python control flow. If `False`, we assume that the layer can
       safely be used to generate a static computation graph.

   Attributes:
     name: The name of the layer (string).
     dtype: The dtype of the layer's weights.
     variable_dtype: Alias of `dtype`.
     compute_dtype: The dtype of the layer's computations. Layers automatically
       cast inputs to this dtype which causes the computations and output to also
       be in this dtype. When mixed precision is used with a
       `tf.keras.mixed_precision.Policy`, this will be different than
       `variable_dtype`.
     dtype_policy: The layer's dtype policy. See the
       `tf.keras.mixed_precision.Policy` documentation for details.
     trainable_weights: List of variables to be included in backprop.
     non_trainable_weights: List of variables that should not be
       included in backprop.
     weights: The concatenation of the lists trainable_weights and
       non_trainable_weights (in this order).
     trainable: Whether the layer should be trained (boolean), i.e. whether
       its potentially-trainable weights should be returned as part of
       `layer.trainable_weights`.
     input_spec: Optional (list of) `InputSpec` object(s) specifying the
       constraints on inputs that can be accepted by the layer.

   We recommend that descendants of `Layer` implement the following methods:

   * `__init__()`: Defines custom layer attributes, and creates layer state
     variables that do not depend on input shapes, using `add_weight()`.
   * `build(self, input_shape)`: This method can be used to create weights that
     depend on the shape(s) of the input(s), using `add_weight()`. `__call__()`
     will automatically build the layer (if it has not been built yet) by
     calling `build()`.
   * `call(self, inputs, *args, **kwargs)`: Called in `__call__` after making
     sure `build()` has been called. `call()` performs the logic of applying the
     layer to the input tensors (which should be passed in as argument).
     Two reserved keyword arguments you can optionally use in `call()` are:
       - `training` (boolean, whether the call is in inference mode or training
         mode). See more details in [the layer/model subclassing guide](
         https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method)
       - `mask` (boolean tensor encoding masked timesteps in the input, used
         in RNN layers). See more details in [the layer/model subclassing guide](
         https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method)
     A typical signature for this method is `call(self, inputs)`, and user could
     optionally add `training` and `mask` if the layer need them. `*args` and
     `**kwargs` is only useful for future extension when more input parameters
     are planned to be added.
   * `get_config(self)`: Returns a dictionary containing the configuration used
     to initialize this layer. If the keys differ from the arguments
     in `__init__`, then override `from_config(self)` as well.
     This method is used when saving
     the layer or a model that contains this layer.

   Examples:

   Here's a basic example: a layer with two variables, `w` and `b`,
   that returns `y = w . x + b`.
   It shows how to implement `build()` and `call()`.
   Variables set as attributes of a layer are tracked as weights
   of the layers (in `layer.weights`).

   ```python
   class SimpleDense(Layer):

     def __init__(self, units=32):
         super(SimpleDense, self).__init__()
         self.units = units

     def build(self, input_shape):  # Create the state of the layer (weights)
       w_init = tf.random_normal_initializer()
       self.w = tf.Variable(
           initial_value=w_init(shape=(input_shape[-1], self.units),
                                dtype='float32'),
           trainable=True)
       b_init = tf.zeros_initializer()
       self.b = tf.Variable(
           initial_value=b_init(shape=(self.units,), dtype='float32'),
           trainable=True)

     def call(self, inputs):  # Defines the computation from inputs to outputs
         return tf.matmul(inputs, self.w) + self.b

   # Instantiates the layer.
   linear_layer = SimpleDense(4)

   # This will also call `build(input_shape)` and create the weights.
   y = linear_layer(tf.ones((2, 2)))
   assert len(linear_layer.weights) == 2

   # These weights are trainable, so they're listed in `trainable_weights`:
   assert len(linear_layer.trainable_weights) == 2
   ```

   Note that the method `add_weight()` offers a shortcut to create weights:

   ```python
   class SimpleDense(Layer):

     def __init__(self, units=32):
         super(SimpleDense, self).__init__()
         self.units = units

     def build(self, input_shape):
         self.w = self.add_weight(shape=(input_shape[-1], self.units),
                                  initializer='random_normal',
                                  trainable=True)
         self.b = self.add_weight(shape=(self.units,),
                                  initializer='random_normal',
                                  trainable=True)

     def call(self, inputs):
         return tf.matmul(inputs, self.w) + self.b
   ```

   Besides trainable weights, updated via backpropagation during training,
   layers can also have non-trainable weights. These weights are meant to
   be updated manually during `call()`. Here's a example layer that computes
   the running sum of its inputs:

   ```python
   class ComputeSum(Layer):

     def __init__(self, input_dim):
         super(ComputeSum, self).__init__()
         # Create a non-trainable weight.
         self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                                  trainable=False)

     def call(self, inputs):
         self.total.assign_add(tf.reduce_sum(inputs, axis=0))
         return self.total

   my_sum = ComputeSum(2)
   x = tf.ones((2, 2))

   y = my_sum(x)
   print(y.numpy())  # [2. 2.]

   y = my_sum(x)
   print(y.numpy())  # [4. 4.]

   assert my_sum.weights == [my_sum.total]
   assert my_sum.non_trainable_weights == [my_sum.total]
   assert my_sum.trainable_weights == []
   ```

   For more information about creating layers, see the guide
   [Making new Layers and Models via subclassing](
     https://www.tensorflow.org/guide/keras/custom_layers_and_models)

   :param kernel: kernel to approximate using a set of random features.
   :param output_dim: total number of basis functions used to approximate
       the kernel.

   .. py:method:: compute_output_shape(self, input_shape: gpflux.types.ShapeType) -> tensorflow.TensorShape

      Computes the output shape of the layer.
      See `tf.keras.layers.Layer.compute_output_shape()
      <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#compute_output_shape>`_.


   .. py:method:: get_config(self) -> Mapping

      Returns the config of the layer.
      See `tf.keras.layers.Layer.get_config()
      <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#get_config>`_.



.. py:class:: RandomFourierFeatures(kernel: gpflow.kernels.Kernel, output_dim: int, **kwargs: Mapping)

   Bases: :py:obj:`RandomFourierFeaturesBase`

   Random Fourier features (RFF) is a method for approximating kernels. The essential
   element of the RFF approach :cite:p:`rahimi2007random` is the realization that Bochner's theorem
   for stationary kernels can be approximated by a Monte Carlo sum.

   We will approximate the kernel :math:`k(\mathbf{x}, \mathbf{x}')`
   by :math:`\Phi(\mathbf{x})^\top \Phi(\mathbf{x}')`
   where :math:`\Phi: \mathbb{R}^{D} \to \mathbb{R}^{M}` is a finite-dimensional feature map.

   The feature map is defined as:

   .. math::

     \Phi(\mathbf{x}) = \sqrt{\frac{2 \sigma^2}{\ell}}
       \begin{bmatrix}
         \cos(\boldsymbol{\theta}_1^\top \mathbf{x}) \\
         \sin(\boldsymbol{\theta}_1^\top \mathbf{x}) \\
         \vdots \\
         \cos(\boldsymbol{\theta}_{\frac{M}{2}}^\top \mathbf{x}) \\
         \sin(\boldsymbol{\theta}_{\frac{M}{2}}^\top \mathbf{x})
       \end{bmatrix}

   where :math:`\sigma^2` is the kernel variance.
   The features are parameterised by random weights:

   - :math:`\boldsymbol{\theta} \sim p(\boldsymbol{\theta})`
     where :math:`p(\boldsymbol{\theta})` is the spectral density of the kernel.

   At least for the squared exponential kernel, this variant of the feature
   mapping has more desirable theoretical properties than its cosine-based
   counterpart :class:`RandomFourierFeaturesCosine` :cite:p:`sutherland2015error`.

   :param kernel: kernel to approximate using a set of random features.
   :param output_dim: total number of basis functions used to approximate
       the kernel.

   .. py:method:: build(self, input_shape: gpflux.types.ShapeType) -> None

      Creates the variables of the layer.
      See `tf.keras.layers.Layer.build()
      <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build>`_.


   .. py:method:: call(self, inputs: gpflow.base.TensorType) -> tensorflow.Tensor

      Evaluate the basis functions at ``inputs``.

      :param inputs: The evaluation points, a tensor with the shape ``[N, D]``.

      :return: A tensor with the shape ``[N, M]``.



.. py:class:: RandomFourierFeaturesCosine(kernel: gpflow.kernels.Kernel, output_dim: int, **kwargs: Mapping)

   Bases: :py:obj:`RandomFourierFeaturesBase`

   Random Fourier Features (RFF) is a method for approximating kernels. The essential
   element of the RFF approach :cite:p:`rahimi2007random` is the realization that Bochner's theorem
   for stationary kernels can be approximated by a Monte Carlo sum.

   We will approximate the kernel :math:`k(\mathbf{x}, \mathbf{x}')`
   by :math:`\Phi(\mathbf{x})^\top \Phi(\mathbf{x}')` where
   :math:`\Phi: \mathbb{R}^{D} \to \mathbb{R}^{M}` is a finite-dimensional feature map.

   The feature map is defined as:

   .. math::
     \Phi(\mathbf{x}) = \sqrt{\frac{2 \sigma^2}{\ell}}
       \begin{bmatrix}
         \cos(\boldsymbol{\theta}_1^\top \mathbf{x} + \tau) \\
         \vdots \\
         \cos(\boldsymbol{\theta}_M^\top \mathbf{x} + \tau)
       \end{bmatrix}

   where :math:`\sigma^2` is the kernel variance.
   The features are parameterised by random weights:

   - :math:`\boldsymbol{\theta} \sim p(\boldsymbol{\theta})`
     where :math:`p(\boldsymbol{\theta})` is the spectral density of the kernel
   - :math:`\tau \sim \mathcal{U}(0, 2\pi)`

   Equivalent to :class:`RandomFourierFeatures` by elementary trignometric identities.

   :param kernel: kernel to approximate using a set of random features.
   :param output_dim: total number of basis functions used to approximate
       the kernel.

   .. py:method:: build(self, input_shape: gpflux.types.ShapeType) -> None

      Creates the variables of the layer.
      See `tf.keras.layers.Layer.build()
      <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build>`_.


   .. py:method:: call(self, inputs: gpflow.base.TensorType) -> tensorflow.Tensor

      Evaluate the basis functions at ``inputs``.

      :param inputs: The evaluation points, a tensor with the shape ``[N, D]``.

      :return: A tensor with the shape ``[N, M]``.



